import tkinter as tk
from tkinter import simpledialog, messagebox
import cv2
import mediapipe as mp
import numpy as np
import json
import os
import time
from PIL import Image, ImageTk, ImageDraw, ImageFont

GESTURES_FILE = "gestures.json"

if not os.path.exists(GESTURES_FILE):
    with open(GESTURES_FILE, "w", encoding="utf-8") as f:
        json.dump({}, f, indent=4, ensure_ascii=False)

with open(GESTURES_FILE, "r", encoding="utf-8") as f:
    try:
        hand_gestures = json.load(f)
    except json.JSONDecodeError:
        hand_gestures = {}

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(False, max_num_hands=1, min_detection_confidence=0.5)

cap = cv2.VideoCapture(0)
sentence = []
last_time_seen = time.time()
instruction_text = "è«‹é»é¸æŒ‰éˆ•æ–°å¢ã€è¾¨è­˜æˆ–åˆªé™¤æ‰‹å‹¢"

def draw_text(frame, text, pos=(30, 30), font_size=28, color=(0, 255, 0)):
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img_pil = Image.fromarray(frame)
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype("msjh.ttc", font_size)
    except:
        font = ImageFont.load_default()
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def update_frame():
    global instruction_text, sentence, last_time_seen
    ret, frame = cap.read()
    if not ret:
        return

    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(image)
    result_text = "æœªåµæ¸¬"
    current_time = time.time()

    if recognizing and results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp.solutions.drawing_utils.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            landmarks = [lm.y for lm in hand_landmarks.landmark[:6]]
            center = np.mean(landmarks[:5])
            code = "".join(['1' if y < center else '0' for y in landmarks])
            result_text = hand_gestures.get(code, "æœªçŸ¥æ‰‹å‹¢")
            if result_text != "æœªçŸ¥æ‰‹å‹¢" and (len(sentence) == 0 or sentence[-1] != result_text):
                sentence.append(result_text)
                last_time_seen = current_time
    elif recognizing:
        if current_time - last_time_seen > 1.5 and sentence:
            print("ğŸ“ å¥å­è¾¨è­˜çµæœï¼š", " ".join(sentence))
            sentence = []

    frame = draw_text(frame, f"{instruction_text}", (30, 20))
    frame = draw_text(frame, f"ç•¶å‰è¾¨è­˜ï¼š{result_text}", (30, 60))
    frame = draw_text(frame, f"çµ„å¥ä¸­ï¼š{' '.join(sentence)}", (30, 100))

    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    imgtk = ImageTk.PhotoImage(image=img)
    video_label.imgtk = imgtk
    video_label.configure(image=imgtk)
    video_label.after(10, update_frame)

def capture_hand_gesture():
    captured = []
    started = False
    frame_count = 0
    global instruction_text
    instruction_text = "ğŸ“¸ åµæ¸¬ä¸­ï¼Œæ“ºå¥½å§¿å‹¢..."

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(image)

        if results.multi_hand_landmarks:
            hand = results.multi_hand_landmarks[0]
            if not started:
                started = True
                frame_count = 0
                instruction_text = "âœ… åµæ¸¬åˆ°æ‰‹å‹¢ï¼Œé–‹å§‹æ‹æ”..."

            if started and frame_count < 5:
                landmarks = [lm.y for lm in hand.landmark[:6]]
                captured.append(landmarks)
                frame_count += 1
                instruction_text = f"ğŸ“· æ‹æ”ä¸­ï¼š{frame_count}/5"
                cv2.waitKey(1000)
            elif frame_count >= 5:
                break
        else:
            instruction_text = "è«‹æ“ºå‡ºä½ çš„æ‰‹å‹¢..."

        cv2.waitKey(100)

    instruction_text = "å®Œæˆï¼å¯ç¹¼çºŒæ–°å¢ã€è¾¨è­˜æˆ–åˆªé™¤"

    if len(captured) == 5:
        avg = np.mean(captured, axis=0)
        center = np.mean(avg[:5])
        code = "".join(['1' if y < center else '0' for y in avg])
        return code
    return None

def add_gesture():
    label = simpledialog.askstring("è¼¸å…¥æ‰‹å‹¢åç¨±", "é€™å€‹æ‰‹å‹¢ä»£è¡¨çš„è©èªï¼ˆä¾‹å¦‚ï¼šä½ å¥½ï¼‰ï¼š")
    if not label:
        return
    code = capture_hand_gesture()
    if code:
        hand_gestures[code] = label
        with open(GESTURES_FILE, "w", encoding="utf-8") as f:
            json.dump(hand_gestures, f, indent=4, ensure_ascii=False)
        messagebox.showinfo("æˆåŠŸ", f"âœ… å·²æ–°å¢æ‰‹å‹¢ï¼š{label}")
    else:
        messagebox.showerror("éŒ¯èª¤", "âŒ æ“·å–å¤±æ•—ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")

def delete_gesture():
    label = simpledialog.askstring("åˆªé™¤æ‰‹å‹¢", "è«‹è¼¸å…¥è¦åˆªé™¤çš„è©èªï¼ˆä¸­æ–‡ï¼‰ï¼š")
    if not label:
        return
    found = False
    for code, word in list(hand_gestures.items()):
        if word == label:
            del hand_gestures[code]
            found = True
            break
    if found:
        with open(GESTURES_FILE, "w", encoding="utf-8") as f:
            json.dump(hand_gestures, f, indent=4, ensure_ascii=False)
        messagebox.showinfo("åˆªé™¤æˆåŠŸ", f"âœ… å·²åˆªé™¤æ‰‹å‹¢ï¼š{label}")
    else:
        messagebox.showerror("æœªæ‰¾åˆ°", f"âŒ æ‰¾ä¸åˆ°æ‰‹å‹¢ï¼š{label}")

def toggle_recognition():
    global recognizing, instruction_text
    recognizing = not recognizing
    if recognizing:
        instruction_text = "ğŸ§  æ‰‹èªè¾¨è­˜å·²å•Ÿå‹•..."
        recog_button.config(text="ğŸ›‘ åœæ­¢è¾¨è­˜")
    else:
        instruction_text = "ğŸ”´ å·²åœæ­¢è¾¨è­˜"
        recog_button.config(text="â–¶ï¸ é–‹å§‹è¾¨è­˜")

root = tk.Tk()
root.title("ä¸€é«”åŒ–æ‰‹èªç³»çµ± GUIï¼ˆå«åˆªé™¤åŠŸèƒ½ï¼‰")
root.geometry("700x580")

video_label = tk.Label(root)
video_label.pack()

button_frame = tk.Frame(root)
button_frame.pack(pady=10)

add_button = tk.Button(button_frame, text="â• æ–°å¢æ‰‹å‹¢", command=add_gesture, bg="lightblue", font=("Arial", 12))
add_button.grid(row=0, column=0, padx=10)

recog_button = tk.Button(button_frame, text="â–¶ï¸ é–‹å§‹è¾¨è­˜", command=toggle_recognition, bg="lightgreen", font=("Arial", 12))
recog_button.grid(row=0, column=1, padx=10)

del_button = tk.Button(button_frame, text="ğŸ—‘ï¸ åˆªé™¤æ‰‹å‹¢", command=delete_gesture, bg="tomato", font=("Arial", 12))
del_button.grid(row=0, column=2, padx=10)

recognizing = False
update_frame()
root.mainloop()

cap.release()
cv2.destroyAllWindows()
