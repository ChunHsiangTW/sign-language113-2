#1å½±ç‰‡è½‰JSON
import os
import cv2
import json
import numpy as np
import mediapipe as mp
from tqdm import tqdm

# === åŸºæœ¬è¨­å®š ===
VIDEO_DIR = "static/datavideos"   # å½±ç‰‡ä¾†æºè³‡æ–™å¤¾
OUTPUT_DIR = "data"               # è¼¸å‡ºJSONè³‡æ–™å¤¾
os.makedirs(OUTPUT_DIR, exist_ok=True)

MAX_FRAMES = 200
FEATURES = 21 * 3 * 2 + 68 * 3  # 330ç¶­ (é›™æ‰‹+è‡‰)

# === åˆå§‹åŒ–Mediapipe ===
mp_hands = mp.solutions.hands
mp_face = mp.solutions.face_mesh
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)
face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=1)

# === è™•ç†æ‰€æœ‰å½±ç‰‡ ===
video_files = [f for f in os.listdir(VIDEO_DIR) if f.lower().endswith((".mp4", ".mov", ".avi"))]
print(f"ğŸ¬ å…±ç™¼ç¾ {len(video_files)} æ”¯å½±ç‰‡ï¼Œé–‹å§‹è™•ç†...")

for video_name in video_files:
    label = os.path.splitext(video_name)[0]
    video_path = os.path.join(VIDEO_DIR, video_name)
    output_json = os.path.join(OUTPUT_DIR, f"{label}.json")

    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frames_data = []

    for fid in tqdm(range(total_frames), desc=f"è™•ç† {video_name}", unit="frame"):
        ret, frame = cap.read()
        if not ret:
            break
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        hand_results = hands.process(frame_rgb)
        face_results = face_mesh.process(frame_rgb)

        # === è‡‰ä¸­å¿ƒ ===
        face_center = np.zeros(3)
        face_landmarks = None
        if face_results.multi_face_landmarks:
            face_landmarks = face_results.multi_face_landmarks[0]
            coords = np.array([[lm.x, lm.y, lm.z] for lm in face_landmarks.landmark[:68]])
            face_center = coords.mean(axis=0)

        # === æ‰‹éƒ¨ç¯€é» ===
        hand_points = []
        if hand_results.multi_hand_landmarks:
            for hand_landmarks in hand_results.multi_hand_landmarks:
                coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
                coords = coords - face_center
                hand_points.extend(coords.flatten().tolist())
        while len(hand_points) < 21 * 3 * 2:
            hand_points.extend([0.0] * (21 * 3))

        # === è‡‰éƒ¨ç¯€é» ===
        face_points = []
        if face_landmarks:
            coords = np.array([[lm.x, lm.y, lm.z] for lm in face_landmarks.landmark[:68]])
            coords = coords - face_center
            face_points = coords.flatten().tolist()
        else:
            face_points = [0.0] * (68 * 3)

        # === åˆä½µ ===
        frame_features = np.array(hand_points + face_points, dtype=np.float32)
        arr3 = frame_features.reshape(-1, 3)
        mean, std = arr3.mean(axis=0), arr3.std(axis=0)
        std[std == 0] = 1
        arr3 = (arr3 - mean) / std
        frames_data.append(arr3.flatten().tolist())

    cap.release()

    # === è£œå¹€æˆ–è£åˆ‡ ===
    if len(frames_data) < MAX_FRAMES:
        pad = MAX_FRAMES - len(frames_data)
        frames_data.extend([[0.0] * FEATURES] * pad)
    else:
        frames_data = frames_data[:MAX_FRAMES]

    # === å„²å­˜ JSON ===
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump({"label": label, "frames": frames_data}, f, ensure_ascii=False, indent=2)
    print(f"âœ… å·²å„²å­˜ï¼š{output_json}")

print("ğŸ‰ æ‰€æœ‰å½±ç‰‡å·²å®Œæˆ 3D é—œéµé»æ“·å–ä¸¦ä»¥è‡‰ç‚ºä¸­å¿ƒå¹³ç§»ï¼")
